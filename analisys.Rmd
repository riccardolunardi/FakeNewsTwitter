---
title: "Fake news su Twitter"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, message = FALSE, warning = FALSE, cache.lazy = FALSE)
```

Durante il periodo di lockdown mi sono chiesto quanto le fake news influenzassero le opinioni delle persone e quanto frequenti fossero le condivisioni di notizie false o ingannevoli sul Web. Quindi, come caso di studio, ho scelto di prendere in causa Twitter e raccogliere più informazioni possibili sui tweet condivisi in questo periodo per rispondere ad alcune domande che mi sono posto.
Tra tutti i social network ho preferito Twitter perché permette, al contrario, ad esempio, di Facebook, di utilizzare alcune API per la ricerca/streaming dei tweet, che mi ha concesso la raccolta dei dati. 

Innanzitutto, com'è possibile discriminare le fake news dalle notizie veritiere?
Putroppo non esiste una maniera universale per capire se una notizia è falsa oppure no. Io mi sono basato sulle "blacklist" di [BUTAC.it](https://www.butac.it/the-black-list/) e di [Bufale.net](https://www.bufale.net/the-black-list-la-lista-nera-del-web/), che riportano a siti che condividono spesso notizie false, modificate in base alle ideologie o pubblicate senza rendere nota alcun tipo di fonte.

Le domande che mi sono posto sono:

- Quanto spesso vengono condivise fake news su Twitter? Quanto vengono retweetate? E che rapporto c'è tra notizie ingannevoli e notizie provenienti da siti affidabili?
- Le notizie che considero false, che tipo di "sentiment" esprimono?
- Quali sono stati gli argomenti più trattati da siti di news non affidabili? Hanno seguito il trend riguardante le notizie delle testate giornalistiche più famose?

Prima di poter rispondere a questi questiti, ho dovuto raccogliere i dati e "ripulirli" per cercare di ridurre le dimensioni del mio dataset

## Come ho costruito il dataset

Il dataset è stato costruito utilizzando la libreria [rtweet](https://rtweet.info/), che permette di interfacciarsi con le API di Twitter tramite R.
In particolare ho usato l'endpoint [statuses/filter](https://developer.twitter.com/en/docs/tweets/filter-realtime/api-reference/post-statuses-filter), che ritorna, per un determinato periodo di tempo, i tweets che rispettano alcuni parametri preimpostati.
Utilizzando quel particolare endpoint, ho raccolto più di 35mila tweet, che non sono tutti i tweet da aprile a oggi, ma solo un campione di essi. Questo è dovuto all' [implementazione delle streaming API di Twitter](https://twittercommunity.com/t/potential-adjustments-to-streaming-api-sample-volumes/31628), le quali ritornano circa l'1% dei tweet pubblici. Quindi non ho ottenuto i dati assoluti, ma una buona rappresentazione di essi.
La raccolta dei dati sarebbe stata possibile anche attraverso l'endpoint [search](https://developer.twitter.com/en/docs/tweets/search/overview), il quale avrebbe ritornato tutti (e quindi non solo un campione) i tweet che rispettavano i parametri dati. Nonostante questi vantaggi, non ho utilizzato questo metodo perché l'API gratuita aveva troppe limitazioni. Infatti ne esistono più versioni: SandBox, Premium e Enterprise. La versione SandBox, quella gratuita, permette di ottenere tweet vecchi di massimo una settimana e con un massimo di 100 tweets per richiesta. Questo avrebbe complicato notevolemente lo sviluppo di uno script.
Le versioni Premium ed Enterpise hanno invece limiti più ampi, [ma possono avere un costo anche notevole in base all'uso che se ne fa](https://developer.twitter.com/en/premium-apis#:~:text=To%20ensure%20it's%20easy%20to,%2499%2Fmonth%20for%20100%20requests.)

Come paramentri ho scelto 10 testate giornalistiche italiane che sono considerate generalmente affidabili (es. Corriere Della Sera, AGI, ANSA.it) e 100 siti d'informazione ritenuti non altrettando corretti.

```{r Streaming, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
api_key <- "your_api_key"
api_secret_key <- "your_api_secret_key"
access_token <- "your_access_token"
access_token_secret <- "your_access_token_secret"

# Autenticazione via API keys
token <- create_token(
  app = "FakeNewsUniUd",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)

# Raccogli lo stream di tweet per un giorno. Readr velocizza il processo di parsing

link_siti_non_affidabili <- "https://raw.githubusercontent.com/riccardolunardi/ProgettoFondamenti/master/websites/websites_fakenews.txt?token=AIZMNO53XQFIZ56OI7BYLOS66CTGM"
siti_non_affidabili <- read.delim(link_siti_non_affidabili, sep = "\n", header = FALSE)
siti_non_affidabili <- as.vector(t(sapply(siti_non_affidabili, tolower))) #t trasforma il dataset in una matrice
siti_non_affidabili <- paste(siti_non_affidabili, collapse = ', ')

link_siti_affidabili <- "https://raw.githubusercontent.com/riccardolunardi/ProgettoFondamenti/master/websites/websites_realnews.txt?token=AIZMNO7S4HTMULYIAU4S5IS66CTHC"
siti_affidabili <- read.delim(link_siti_affidabili, sep = "\n", header = FALSE)
siti_affidabili <- as.vector(t(sapply(siti_affidabili, tolower))) #t trasforma il dataset in una matrice
siti_affidabili <- paste(siti_affidabili, collapse = ', ')

siti=paste(siti_affidabili,siti_non_affidabili, collapse = ', ')

stream_tweets(
  siti,
  timeout = 60*60*24*7, #Una settimana
  file_name = paste("data/day", Sys.Date(), sep=""),
  parse = TRUE
)

```

Il file prodotto dalla funzione `stream_tweets` è un documento che raccoglie più JSON, uno per tweet raccolto. Avendo avuto solo qualche mese per raccogliere i dati, ogni giorno facevo ripartire lo streaming per evitare possibili bug o stalli dello script, per cui alla fine ho ottenuto un insieme di JSON, ognuno rinominato con il giorno in cui è stato creato.

Per trasformare il JSON in un dataframe ho utilizzato la funzione parse_stream di rtweet, che automatizza questo processo. La funzione però accetta solo un file per volta, quindi ho creato un array in cui ogni elemento era un path di JSON e iterare la cosidetta funzione per ogni file.

```{r Raccolta dei dati, echo=TRUE}
library(rtweet)
library(stringr)
library(readr)

#CODICE DELL'UNIONE DEI JSON
get_tweets_by_date = function(start_date, leng) {
  link1 = "data/day" #prefisso
  
  month=seq(as.Date(start_date), by = "day", length.out = leng) 
  #Ottengo una sequenza di lunghezza leng di date con lo stesso formato di start_date 

  links = str_c(link1, month, ".json") #Unione prefisso + data + suffisso
  
  links
}

#Sequenza dei giorni di aprile + doppione
april=c(get_tweets_by_date("2020-04-22",9),("data/day2020-04-24-01.json"))

#Sequenza dei giorni di maggio - 2 giorni
may=get_tweets_by_date("2020-05-01",31)
may=may[!may %in% c("data/day2020-05-24.json","data/day2020-05-05.json")]

#Sequenza dei giorni di aprile + doppione
june=get_tweets_by_date("2020-06-01",29)
june=june[!june %in% c("data/day2020-06-19.json", "data/day2020-06-24.json", "data/day2020-06-06.json", "data/day2020-06-05.json")]

every_json_path = c(april, may, june) #Unione dei vettori

# Creo un vettore con lunghezza predefinita
stream_analizzate <- vector("list",length(every_json_path))  

for (i in 1:length(every_json_path)) {        
  #print(every_json_path[i])
  stream_analizzate[[i]] <- parse_stream(every_json_path[i])          
}

#stream_analizzate[[1]]

```

I file relativi ad alcuni giorni sono stati rimossi per i seguenti motivi: lo script si è bloccato, il server ha interrotto lo streaming, i dati mancanti sono inclusi nel file del giorno precedente.

Ottenuta la lista _stream_analizzate_, contenente un dataset per ogni giorno, utilizzo `do.call`, che mi permette di usare `rbind` su una lista, in modo da unirle tutti gli elementi in uno stesso dataframe.

```{r Generazione dataset, echo=TRUE}
#Applico rbind alla lista di dataframe
everytweet <- do.call("rbind", stream_analizzate)

#Dataframe completo
#View(everytweet)

#Infomazioni sul dataframe
ncol(everytweet)
nrow(everytweet)
object.size(everytweet)
```

Come possiamo notare ci sono molte colonne e altrettante righe nel dataset, che lo rendendono così molto pesante, circa 90MB.
Conviene cercare di ridurlo di dimensioni per agevolare l'elaborazione.

## Pulizia dei dati

Tra i 90 campi del dataset, solo alcuni di questi sono utili a rispondere alle domande che mi sono posto. Sono da rimuovere infatti tutti i campi relativi alle immagini, agli hashtag, ai follower, ai favorites, etc...

_campi\_utili_ conterrà solo gli attributi necessari

```{r Campi utili, echo=TRUE}
campi_utili = c("user_id","status_id","created_at","screen_name","text", 
                "urls_url", "urls_expanded_url","retweet_status_id","retweet_text",
                "retweet_user_id","is_retweet")
```

L'obiettivo di questa pulizia è di avere 2 dataset:
(1) Uno in cui elimino solo le colonne che non servono
(2) Uno in cui si hanno solo i tweets, ma con un nuovo attributo che specifica quante volte sono stati retweetati.

Sarebbe comodo ed efficiente avere solo il secondo dataset, ma così si perderebbero alcune informazioni, come la data di un retweet, le parole aggiunte al retweet quando è stato condiviso, etc; quindi sarà preferibile usare quello più leggero quando possibile.

E' necessario quindi effettuare il conteggio dei retweet perché lo streaming ritorna il numero corrente di retweet, che ovviamente all'inizio è 0. Avendo anche i retweet, però, è possibile calcolarlo.

Qui creo il dataframe del punto **(1)**

```{r Primo dataframe, echo=TRUE}
library(dplyr)
library(anytime)

clean_tweets = everytweet %>%
  select(campi_utili,retweet_created_at)

ncol(clean_tweets)
nrow(clean_tweets)
object.size(clean_tweets)
```

Mentre qui il dataframe **(2)**

```{r Secondo dataframe, echo=TRUE}

#Ottengo solo retweet
retweets <- everytweet %>%
  select(campi_utili,retweet_created_at) %>%
  filter(is_retweet)

#Ottengo solo tweet
tweets <- everytweet %>%
  select(campi_utili, -retweet_user_id, -retweet_text, -retweet_status_id) %>%
  filter(!is_retweet) 


#Dataframe di retweet con contatore aggiornato
retweets_with_number <- retweets %>%
  select(retweet_status_id,retweet_created_at,retweet_text,retweet_user_id,is_retweet) %>%
  group_by(retweet_status_id) %>%
  mutate(retweet_counter = n()) %>%
  distinct(retweet_status_id, .keep_all=TRUE) %>%
  arrange(retweet_counter)

tweets_with_retweet_number <- full_join(tweets, retweets_with_number, by=c("status_id" = "retweet_status_id"))

tweets_with_retweet_number <- tweets_with_retweet_number %>%
  mutate(tmp_tweet_text = ifelse(is.na(text), retweet_text, text),
         tmp_created_at = anytime(ifelse(is.na(created_at), retweet_created_at, created_at)), 
         #In questo passaggio la data viene convertita in UNIX time, lo riconverto in tempo "umano" per una lettura migliore dei dati
         tmp_tweet_user_id = ifelse(is.na(user_id), retweet_user_id, user_id)) %>%
  #Elimino campi che non servono più (alcuni verranno rimpiazzati dei temporanei)
  select(-user_id, -created_at, -text, -is_retweet.x, -is_retweet.y, -retweet_text, -retweet_user_id, -retweet_created_at) %>%
  rename(user_id=tmp_tweet_user_id, text=tmp_tweet_text, created_at=tmp_created_at) %>%
  arrange(status_id, user_id, created_at, screen_name, text)


#Riordino le colonne
tweets_with_retweet_number <- tweets_with_retweet_number[c(1,8,7,2,6,3,4,5)] %>%
  mutate(retweet_counter = ifelse(is.na(retweet_counter), 0, retweet_counter))


#View(tweets_with_retweet_number)

ncol(tweets_with_retweet_number)
nrow(tweets_with_retweet_number)
object.size(tweets_with_retweet_number)

```

Se avessi usato `inner_join` avrei avuto come dataset risultante solo i tweet che sono stati retweetati durante il mio monitoraggio.
Se avessi usato `left_join` avrei ottenuto i tweet retweetati e quelli non retweetati.
Se avessi usato `right_join` avrei avuto solo i tweet che sono stati condivisi e solo i tweet che sono stati condivisi prima del monitoraggio. 

`full_join` invece evita incogruenze che si sarebbero potute incontrare con alcuni retweet. Infatti, se un tweet che è stato twittato prima del mio monitoraggio fosse stato ricondiviso durante la raccolta dei tweet, non sarei stato in grado di ottenere informazioni sul tweet iniziale.

Ora emerge la necessità di aggiungere un campo al dataframe che evidenzi in modo veloce se la notizia proviene da una fonte affidabile o no.

```{r Creazione del campo di affidabilità, echo=TRUE}
#Leggiamo i siti non affidabili dal file dedicato e creiamo un vettore con questi siti
link_siti_non_affidabili <- "websites/websites_fakenews.txt"
siti_non_affidabili <- read.delim(link_siti_non_affidabili, sep = "\n", header = FALSE)
siti_non_affidabili <- as.vector(t(sapply(siti_non_affidabili, tolower))) #t trasforma il dataset in una matrice

tidy_tweets <- clean_tweets %>%
  mutate(urls_url = ifelse(as.character(urls_url)=="NULL", NA, sapply(urls_url, tolower))) %>%
  rowwise() %>% 
  mutate(is_affidabile = !any(urls_url %in% siti_non_affidabili))

nrow(tidy_tweets %>% filter(!is_affidabile && !is_retweet))
nrow(tidy_tweets %>% filter(is_affidabile && !is_retweet))
```

Dal risultato ottenuto appare evidente che le notizie false sono in netta minoranza.

Creando un grafico a barre, diviso per giorni, è ben visibile il rapporto che hanno le fake news rispetto alle notizie che provengono da fonti affidabili.
In questa fase utilizzo il dataframe contente tweet e retweet, visto che è necessario sapere anche **quando** è avvenuto un retweet

```{r Grafico n°1, fig.width=18, fig.height=12, echo=TRUE}
library(ggplot2)

tweets_data_for_plotting_affidabili <- tidy_tweets %>%
  filter(as.Date(created_at)>"2020-04-22", as.Date(created_at)>"2020-07-02") %>%
  mutate(created_at=substr(created_at,0,10),
         categoria=if_else(is_affidabile && is_retweet,
                    "affidabile_retweet",
                    if_else(is_affidabile && !is_retweet,
                      "affidabile_tweet",
                     if_else(!is_affidabile && is_retweet,
                        "non_affidabile_retweet",
                        "non_affidabile_tweet")))
  )

ggplot(tweets_data_for_plotting_affidabili) +
  geom_bar(aes(x = as.Date(created_at), fill = categoria)) +
  labs(title = "Quanto frequentemente vengono condivise le notizie?", x="Data del tweet", y="n° di tweet") +
  scale_fill_manual(values=c("#17bf63", "#1da1f2", "#f2a71d", "#f24f1d"), name="Tweets", labels=c("Tweet","Retweet","Retweet di notizie non affidabili","Tweet di notizie non affidabili")) +
  scale_x_date(date_labels = "%m-%d", date_breaks = "2 days") +
  theme_minimal()

```

Come dimostrato dal precedente risultato, le fake news vengono tweetate e retweetate molto meno rispetto alle altre. 
Possiamo notare però come ci sia stato un periodo insolito in cui le notizie provenienti da siti non affidabili giravano più frequentemente del normale.
Questo periodo va dal 25 aprile al 12 maggio e corrisponde all'ultimo momento di lockdown.
La mia interpretazione di questo fenomeno è che negli utenti, dopo la prolungata permanenza in casa, si siano concentrate paure e ansie del momento, tanto da provare ad attenuarle con un rimedio apparente: la condvisione ossessiva di notizie dal click facile e create appositamente per questo.
Dopo la fase di quarantena il numero di fake news si è assestato ed è sempre rimasto basso da allora.

Segue la presentazione grafica dei dati in percentuale per ogni giorno

```{r Grafico n°2, fig.width=18, fig.height=12, echo=TRUE}
tweets_data_for_plotting_affidabili2 <- tweets_data_for_plotting_affidabili %>%
  count(created_at, is_affidabile) %>%
  group_by(created_at) %>%
  mutate(tot=sum(n),conto = n/tot)
  

ggplot(data = tweets_data_for_plotting_affidabili2, mapping = aes(y = n/tot, x = as.Date(created_at), color = factor(is_affidabile))) +
  geom_point() + 
  geom_smooth() +
  labs(title = "Quanto frequentemente vengono condivise le notizie?", x="Data del tweet", y="n° di tweet") +
  scale_color_manual(values=c("#f24f1d","#17bf63"), name="Tweets", labels=c("Tweet affidabili","Tweet non affidabili")) +
  scale_x_date(date_labels = "%m-%d", date_breaks = "2 days") +
  theme_minimal() 

```

Emerge ancora chiaramente che in quel periodo circolassero più notizie non affidabili; ma successivamente, a partire dal 5 maggio, c'è stata una riduzione, che si è assestata verso il 20 maggio senza mai riprendere il vecchio trend.

##Analisi del sentiment

Per l'analisi del sentiment ho usato [TextWiller](https://github.com/livioivil/TextWiller), una libreria di utilities dedicata alla lingua italiana. [Qui](https://github.com/livioivil/TextWiller/blob/master/README.md) le istruzioni per scaricarla ed installarla.

Utilizzando la funzione `sentiment(text)`, viene fatta una normalizzazione del testo e calcolato un punteggio: -1 (Negativo), 0 (Neutrale), 1 (Positivo), in base alla possibilità o meno di esprimere un'emozione.
Applicando questa funzione al dataset si comprende che emozione condivide il titolo dell'articolo e/o il testo aggiunto dall'utente prima di twittare.

Il costo computazionale di questo calcolo è molto alto: ad esempio un Intel i5-4590 impiega quasi 8 minuti a elaborare la parte di codice sottostante.

```{r Analisi del sentiment, echo=FALSE, message=FALSE, warning=FALSE}
library(TextWiller)

tweets_with_retweet_number_upd <- tweets_with_retweet_number %>%
  mutate(urls_url = ifelse(as.character(urls_url)=="NULL", NA, sapply(urls_url, tolower))) %>%
  rowwise() %>% 
  mutate(is_affidabile = !any(urls_url %in% siti_non_affidabili))

tweets_with_retweet_number_upd <- tweets_with_retweet_number_upd %>% 
  mutate(sentiment = sentiment(text))

#View(tweets_with_retweet_number_upd)

tweets_with_retweet_number_upd_plot <- tweets_with_retweet_number_upd %>%
  filter(as.Date(created_at)>"2020-04-22", as.Date(created_at)>"2020-07-02")

ggplot(tweets_with_retweet_number_upd_plot) +
  geom_bar(aes(x = sentiment, fill = is_affidabile), position = position_dodge()) +
  labs(title = "Sentiment Analysis", x="Sentiment", y="n° di tweet") +
  scale_x_continuous(breaks=-1:1, labels=c("Negativo (-1)", "Neutrale (0)", "Positivo (1)")) +
  scale_fill_manual(values=c("#f2a71d", "#1da1f2"), name="Tweets", labels=c("Non affidabili","Affidabili")) +
  theme_minimal()


```

In entrambe le categorie di notizie, le più condivise sono quelle con un titolo o una descrizione negativa.
Questo rispetta quello che è stato scoperto da varie ricerche (es. [articolo della BBC](https://www.bbc.com/future/article/20140728-why-is-all-the-news-bad)), cioè che l'essere umano è più propenso a dare più attenzione a notizie negative che a quelle positive.

La neutralità indica notizie che sono postate solamente con il titolo e quest'ultimo non aveva particolari connotazioni negative o positive.

L'analisi del sentiment attraverso un vocabolario va sempre trattata con cautela, perché ad esempio non considera il sarcasmo o insiemi di parole; in più il testo che elabora è molto poco: una o due frasi al massimo per tweet. Detto questo, ritengo sia comunque un'analisi affidabile visto i risultati ottenuti.

```{r Controllo sentimento (manuale), eval=FALSE, include=FALSE}
View(tweets_with_retweet_number_upd %>% 
       select(text, is_affidabile, sentiment) %>%
       filter(is_affidabile, sentiment==0)
     )
```

##Trend a confronto

Come ultima analisi, valuto gli argomenti su cui hanno puntato le fake news e se hanno seguito il trend delle notizie più in voga del momento.

```{r Generazione wordclouds, echo=TRUE}
library(tidytext)
library(wordcloud)
library(tm)

stop_words_extra = c("t.co","https","lkhl0j1pqm","wgyypsdiqo","lkhl0ike2c","tpm3unabk9","amp","2","uc8qrmsbx2","weieh1n3qy","wxsqwmbw67","33iqc7xzjo","fispjyrtyw","bgfnwd339h","rxmeve8hzy","lty4aseqyc","wdfy92x5rd","ykmiyz3fm3")
stop_words_vector= append(stop_words_extra, stopwords(kind = "it"), after = length(stop_words_extra))
stop_words_ita = data.frame(word = stop_words_vector)


text_mining_f_tweets <- tweets_with_retweet_number_upd %>%
  filter(!is_affidabile) %>%
  unnest_tokens(word, text) %>%
  select(-urls_expanded_url) %>%
  anti_join(stop_words_ita)

text_mining_t_tweets <- tweets_with_retweet_number_upd %>%
  filter(is_affidabile) %>%
  unnest_tokens(word, text) %>%
  select(-urls_expanded_url) %>%
  anti_join(stop_words_ita)

#Il select è necessario perché dovuto ad un bug nella versione di dplyr 1.0.0 (funziona correttamente nella versione 0.8.5: https://stackoverflow.com/questions/62609723/must-subset-elements-with-a-valid-subscript-vector-during-an-dplyranti-join)

text_mining_f_tweets %>%
  count(word, sort = TRUE) %>%
  filter(n>20) %>%
  with(wordcloud(word, n, max.words = 30, rot.per = 0, random.order=FALSE, random.color=FALSE, colors=brewer.pal(8, "Dark2")))

text_mining_t_tweets %>%
  count(word, sort = TRUE) %>%
  filter(n>20) %>%
  with(wordcloud(word, n, max.words = 30, rot.per = 0, random.order=FALSE, random.color=FALSE, colors=brewer.pal(8, "Dark2")))
  

```

Entrambi i tipi di notizia si focalizzano sulla questione COVID-19 e hanno argomenti molto simili tra di loro. Le fake news però si concentrano più sulla politica e i rapporti con gli altri paesi ("MES", "Trump", "Germania", "Cina", etc...), mentre le altre notizie più sul virus e sul suo effetto in Italia ("Coronavirus", "Lombardia", "Fase", etc...)
Da questo si deduce che più che seguire passo-passo il trend delle notizie verificate, le fake news hanno cavalcato altri avvenimenti attuali per fare più click.

Ho scelto di trattare le fake news poiché è un argomento che mi incuriosisce molto; mi sarei aspettato che questo tipo di notizie si diffondessero con maggior frequenza. Ho invece constatato che, almeno per Twitter, sembra non essere così.

```{r Dummy section, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

```

