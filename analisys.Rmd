---
title: "FakeNews on Twitter"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, message = FALSE, warning = FALSE)
```

Durante il periodo di lockdown mi sono chiesto quanto le fake news influenzassero le opinioni delle persone e quanto frequenti fossero le condivisioni di notizie false o ingannevoli sul Web. Per questo, come progetto, ho scelto di prendere in causa Twitter e raccogliere più informazioni possibili su i tweet condivisi per rispondere ad alcune domande che mi sono posto.

Perché Twitter?
Ho scelto Twitter perché permette, al contrario di Facebook, di utilizzare alcune API per la ricerca/streaming di tweet, che ha permesso la raccolta dei dati. 

Come faccio ad essere sicuro che siano fake news?
Putroppo non esiste una maniera universale per capire se una notizia è fake oppure no. Io mi sono basato sulle "blacklist" di [BUTAC.it](https://www.butac.it/the-black-list/) e di [Bufale.net](https://www.bufale.net/the-black-list-la-lista-nera-del-web/), che riportano siti di notizie che hanno spesso condiviso notizie false, modificate in base alle ideologie o senza allegare alcun tipo di fonte.


Le domande che mi sono posto sono:

- Quanto spesso vengono condivise fake news su Twitter? Quanto vengono retweetate? EChe rapporto c'è tra notizie ingannevoli e notizie provenienti da siti affidabili?
- Le notizie che considero false, che tipo di "sentiment" esprimono?
- Quali sono stati gli argomenti più trattati da siti di news non affidabili?

Prima di poter rispondere a questi questiti, ho dovuto raccogliere i dati e "ripulirli" per cercare di ridurre le dimesioni del mio dataset

## Come ho costruito il dataset

Il dataset è stato costruito utilizzando la libreria [rtweet](https://rtweet.info/), che permette di interfacciarsi con le API di Twitter tramite R.
In particolare ho usato l'endpoint [statuses/filter](https://developer.twitter.com/en/docs/tweets/filter-realtime/api-reference/post-statuses-filter), che ritorna, per un determinato periodo di tempo, tweet che rispettano alcuni parametri preimpostati.
I tweet ritornati non sono tutti i tweet che rispettano i parametri, ma solo un campione. Quindi non abbiamo i dati assoluti, ma una buona rappresentazione di essi.

```{r Streaming, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
api_key <- "your_api_key"
api_secret_key <- "your_api_secret_key"
access_token <- "your_access_token"
access_token_secret <- "your_access_token_secret"

## Autenticazione via API keys
token <- create_token(
  app = "FakeNewsUniUd",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)

## Raccogli lo stream di tweet per un giorno. Readr velocizza il processo di parsing

link_siti_non_affidabili <- "https://raw.githubusercontent.com/riccardolunardi/ProgettoFondamenti/master/websites/websites_fakenews.txt?token=AIZMNO53XQFIZ56OI7BYLOS66CTGM"
siti_non_affidabili <- read.delim(link_siti_non_affidabili, sep = "\n", header = FALSE)
siti_non_affidabili <- as.vector(t(sapply(siti_non_affidabili, tolower))) #t trasforma il dataset in una matrice
siti_non_affidabili <- paste(siti_non_affidabili, collapse = ', ')

link_siti_affidabili <- "https://raw.githubusercontent.com/riccardolunardi/ProgettoFondamenti/master/websites/websites_realnews.txt?token=AIZMNO7S4HTMULYIAU4S5IS66CTHC"
siti_affidabili <- read.delim(link_siti_affidabili, sep = "\n", header = FALSE)
siti_affidabili <- as.vector(t(sapply(siti_affidabili, tolower))) #t trasforma il dataset in una matrice
siti_affidabili <- paste(siti_affidabili, collapse = ', ')

siti=paste(siti_affidabili,siti_non_affidabili, collapse = ', ')
#siti

stream_tweets(
  siti,
  timeout = 60*60*24*7, #Una settimana
  file_name = paste("data/day", Sys.Date(), sep=""),
  parse = TRUE
)

```

Il file prodotto da _stream_tweets_ è un file che raccoglie un JSON per tweet raccolto. Avendo avuto solo quale mese per raccogliere i dati, ogni giorno facevo ripartire lo streaming per evitare possibili bug o stalli dello script, per cui alla fine ho ottenuto un insieme di JSON, ogniuno rinominato con il giorno in cui è stato creato quel file.

Per trasformare il JSON in un dataframe ho utilizzato la funzione parse_stream di rtweet, che automatizza questo processo. La funzione però, accetta solo un file per volta, quindi ho dovuto creare un array in cui ogni elemento era un path di JSON e iterare la cosidetta funzione per ogni file.

```{r}
library(rtweet)
library(stringr)
library(readr)

#CODICE DELL'UNIONE DEI JSON
get_tweets_by_date = function(start_date, leng) {
  link1 = "data/day" #prefisso
  
  month=seq(as.Date(start_date), by = "day", length.out = leng) 
  #Ottengo una sequenza di lunghezza leng di date con lo stesso formato di start_date 

  links = str_c(link1, month, ".json") #Unione prefisso + data + suffisso
  
  links
}

#Sequenza dei giorni di aprile + doppione
april=c(get_tweets_by_date("2020-04-22",9),("data/day2020-04-24-01.json"))

#Sequenza dei giorni di maggio - 2 giorni
may=get_tweets_by_date("2020-05-01",31)
may=may[!may %in% c("data/day2020-05-24.json","data/day2020-05-05.json")] #Tolgo questi due giorni per un errore di esecuzione dello script

#Sequenza dei giorni di aprile + doppione
june=get_tweets_by_date("2020-06-01",24)
june=june[!june %in% c("data/day2020-06-19.json", "data/day2020-06-24.json", "data/day2020-06-06.json", "data/day2020-06-05.json")] #Tolgo questo giorno perché è incluso nel file day2020-06-19.json

every_json_path = c(april, may, june) #Unione dei vettori

# Creo un vettore con lunghezza predefinita
stream_analizzate <- vector("list",length(every_json_path))  

for (i in 1:length(every_json_path)) {        
  #print(every_json_path[i])
  stream_analizzate[[i]] <- parse_stream(every_json_path[i])          
}

#stream_analizzate[[1]]

```

Adesso, avendo la lista _stream_analizzate_ contenente un dataset per ogni giorno, per
Per unirle tutte in uno stesso dataframe, utilizzo do.call, che mi permette di usare rbind su una lista

```{r}
#Applico rbind alla lista di dataframe
everytweet <- do.call("rbind", stream_analizzate)

#Dataframe completo
#View(everytweet)

#Infomazioni sul dataframe
ncol(everytweet)
nrow(everytweet)
object.size(everytweet)
```
Come possiamo notare ci sono molte colonne e altrettante righe nel dataset, rendendolo così molto pesante, sui 90MB.
Conviene cercare di ridurlo di dimesioni per agevolare l'elaborazione.

NOTE:
La raccolta dei dati è sarebbe stata possibile anche attraverso l'endpoint [search](https://developer.twitter.com/en/docs/tweets/search/overview), il quale avrebbe ritornato tutti (e quindi non solo un campione) i tweet che rispettavano i parametri dati. Nonostante questi vantaggi, non ho utilizzato questo metodo perché l'API free aveva troppe limitazioni. Infatti ne esistono più versioni: SandBox, Premium e Enterprise. La versione SandBox, quella gratuita, permette di ottenere tweet vecchi di massimo una settimana e con un massimo di 100. Questo avrebbe complicato notevolemente lo sviluppo di uno script.
Le versioni Premium ed Enterpise hanno invece limiti più ampi, [ma vengono a costare anche parecchio in base all'uso che se ne fa](https://developer.twitter.com/en/premium-apis#:~:text=To%20ensure%20it's%20easy%20to,%2499%2Fmonth%20for%20100%20requests.)

## Pulizia dei dati

Tra i 90 campi del dataset, solo alcuni di questi sono utili a rispondere alle domande poste inizialmente. Rimuoveremo infatti tutti i campi realtivi alle immagini, agli hashtag, ai follower, ai favorites, etc...

_campi\_utili_ conterrà solo gli attributi necessari

```{r}
campi_utili = c("user_id","status_id","created_at","screen_name","text", 
                "urls_url", "urls_expanded_url","retweet_status_id","retweet_text",
                "retweet_user_id","is_retweet")
```

L'obiettivo è quello di avere 2 dataset:
(1) Uno in cui elimino solo le colonne che non servono
(2) Uno in cui abbiamo solo i tweet condivisi, con un nuovo attributo che specifica quanto è stato retweetato quel tweet.

Sarebbe comodo ed efficiente avere solo il secondo, però in quel caso perdiamo alcune informazioni, come la data di un retweet, se sono state aggiunte delle parole ad esso, etc, quindi cercheremo di usare quello più leggero quando possibile

Dobbiamo effettuare il conteggio dei retweet perché lo streaming ritorna il numero corrente di retweet, che ovviamente all'inizio è 0. Avendo anche i retweet, però, possiamo farlo da noi.

Qui creiamo il dataframe del punto (1)


```{r}
library(dplyr)
library(anytime)

clean_tweets = everytweet %>%
  select(campi_utili,retweet_created_at)

ncol(clean_tweets)
nrow(clean_tweets)
object.size(clean_tweets)
```

Metre qui il dataframe (2)

```{r}

#Ottengo solo retweet
retweets <- everytweet %>%
  select(campi_utili,retweet_created_at) %>%
  filter(is_retweet)

#Ottengo solo tweet
tweets <- everytweet %>%
  select(campi_utili, -retweet_user_id, -retweet_text, -retweet_status_id) %>%
  filter(!is_retweet) 


#Dataframe di retweet con contatore aggiornato
retweets_with_number <- retweets %>%
  select(retweet_status_id,retweet_created_at,retweet_text,retweet_user_id,is_retweet) %>%
  group_by(retweet_status_id) %>%
  mutate(retweet_counter = n()) %>%
  distinct(retweet_status_id, .keep_all=TRUE) %>%
  arrange(retweet_counter)

tweets_with_retweet_number <- full_join(tweets, retweets_with_number, by=c("status_id" = "retweet_status_id"))

tweets_with_retweet_number <- tweets_with_retweet_number %>%
  mutate(tmp_tweet_text = ifelse(is.na(text), retweet_text, text),
         tmp_created_at = anytime(ifelse(is.na(created_at), retweet_created_at, created_at)), 
         #In questo passaggio la data viene convertita in UNIX time, lo riconverto in tempo "umano" per una lettura migliore dei dati
         tmp_tweet_user_id = ifelse(is.na(user_id), retweet_user_id, user_id)) %>%
  #Elimino campi che non servono più (alcuni verranno rimpiazzati dei temporanei)
  select(-user_id, -created_at, -text, -is_retweet.x, -is_retweet.y, -retweet_text, -retweet_user_id, -retweet_created_at) %>%
  rename(user_id=tmp_tweet_user_id, text=tmp_tweet_text, created_at=tmp_created_at) %>%
  arrange(status_id, user_id, created_at, screen_name, text)


#Riordino le colonne
tweets_with_retweet_number <- tweets_with_retweet_number[c(1,8,7,2,6,3,4,5)] %>%
  mutate(retweet_counter = ifelse(is.na(retweet_counter), 0, retweet_counter))


#View(tweets_with_retweet_number)

ncol(tweets_with_retweet_number)
nrow(tweets_with_retweet_number)
object.size(tweets_with_retweet_number)

```

Se avessi usato `inner_join` avrei avuto come dataset risultante solo tweet che sono stati ritweetati durante il mio monitoraggio
Se avessi usato `left_join` avrei ottenuto i tweet retweetati e quelli non retweetati
Se avessi usato `right_join` avrei avuto solo tweet che sono stati condivisi e solo tweet che sono stati condivisi prima del monitoraggio. 

`full_join` invece evita incogruenze che si sarebbero potute incontrare con alcuni retweet. Infatti, se un tweet che è stato twittato prima del mio monitoraggio fosse stato ricondiviso durante la raccolta dei tweet, non sarei stato in grado di ottenere informazioni sul tweet iniziale.

Ora abbiamo la necessità di aggiungere un campo al dataframe che ci faccia sapere in modo veloce se la notizia proviene da una fonte affidabile o no

```{r}
#Leggiamo i siti non affidabili dal file dedicato e creiamo un vettore con questi siti
link_siti_non_affidabili <- "websites/websites_fakenews.txt"
siti_non_affidabili <- read.delim(link_siti_non_affidabili, sep = "\n", header = FALSE)
siti_non_affidabili <- as.vector(t(sapply(siti_non_affidabili, tolower))) #t trasforma il dataset in una matrice

tidy_tweets <- clean_tweets %>%
  mutate(urls_url = ifelse(as.character(urls_url)=="NULL", NA, sapply(urls_url, tolower))) %>%
  rowwise() %>% 
  mutate(is_affidabile = !any(urls_url %in% siti_non_affidabili))

nrow(tidy_tweets %>% filter(!is_affidabile && !is_retweet))
nrow(tidy_tweets %>% filter(is_affidabile && !is_retweet))
```


Da questo risultato possiamo già vedere che chiaramente le notizie false sono in netta minoranza.
Creiamo un grafico a barre diviso per giorni per vedere che rapporto hanno le fake news rispetto alle notizie che provengono da fonti affidabili.
In questa fase utilizziamo il dataframe contente tweet e retweet, visto che abbiamo bisogno di sapere anche _quando_ è avvenuto un retweet

```{r}
library(ggplot2)

tweets_data_for_plotting_affidabili <- tidy_tweets %>%
  filter(as.Date(created_at)>"2020-04-22") %>%
  mutate(created_at=substr(created_at,0,10),
         categoria=if_else(is_affidabile && is_retweet,
                    "affidabile_retweet",
                    if_else(is_affidabile && !is_retweet,
                      "affidabile_tweet",
                     if_else(!is_affidabile && is_retweet,
                        "non_affidabile_retweet",
                        "non_affidabile_tweet")))
  )

ggplot(tweets_data_for_plotting_affidabili) +
  geom_bar(aes(x = as.Date(created_at), fill = categoria)) +
  labs(title = "Quanto frequentemente vengono condivise le notizie?", x="Data del tweet", y="n° di tweet") +
  scale_fill_manual(values=c("#17bf63", "#1da1f2", "#f2a71d", "#f24f1d"), name="Tweets", labels=c("Tweet","Retweet","Retweet di notizie non affidabili","Tweet di notizie non affidabili")) +
  scale_x_date(date_labels = "%m-%d", date_breaks = "2 days") +
  theme_minimal()

#View(tweets_data_for_plotting_affidabili %>% filter(!is_affidabile))

```

Come previsto, le fake news vengono tweetate e retweetate molto meno rispetto alle altre. 
Possiamo notare però come ci sia stato uno strano periodo in cui le notizie provenienti da siti non affidabili giravano più frequentemente rispetto al solito.
Questo periodo va dal 25 aprile al 12 maggio, che corrisponde all'ultimo periodo di lockdown. 
La mia interpretazione di questo è che le persone, rimenendo a casa, si siano fatte prendere dall'ansia e dalla paura del momento, credendo e condividendo notizie fasulle o poco precise
Dopo la fase di quarantena il numero di fake news si è essestato ed è sempre rimasto basso.

Adesso plottiamo i dati in percentuale per ogni giorno

```{r}

tweets_data_for_plotting_affidabili2 <- tweets_data_for_plotting_affidabili %>%
  count(created_at, is_affidabile) %>%
  group_by(created_at) %>%
  mutate(tot=sum(n),conto = n/tot)
  

ggplot(data = tweets_data_for_plotting_affidabili2, mapping = aes(y = n/tot, x = as.Date(created_at), color = factor(is_affidabile))) +
  geom_point() + 
  geom_smooth() +
  labs(title = "Quanto frequentemente vengono condivise le notizie?", x="Data del tweet", y="n° di tweet") +
  scale_color_manual(values=c("#f24f1d","#17bf63"), name="Tweets", labels=c("Tweet affidabili","Tweet non affidabili")) +
  scale_x_date(date_labels = "%m-%d", date_breaks = "2 days") +
  theme_minimal() 

```

Anche da qui è chiaro che in quel periodo ci fossero più notizie non affidabili, ma col tempo, a partire dal 5 maggio, c'è stata una riduzione, che si è assestata verso il 20 maggio senza mai riprendere il vecchio trend

##Analisi del sentiment

Per l'analisi del sentimento ho usato [TextWiller](https://github.com/livioivil/TextWiller), una libreria di utilities dedicata alla lingua italiana. [Qui](https://github.com/livioivil/TextWiller/blob/master/README.md) le istruzioni per scaricarla.

Utilizzando la funzione `sentiment(text)`, viene fatta una normalizzazione testo passato e calcolato un punteggio: -1 (Negativo), 0 (Neutrale), 1 (Positivo).
Applichiando questa funzione al dataset possiamo capire che sentimento condivide il titolo dell'articolo e/o il testo aggiunto dall'utente prima di twittare


```{r}
library(TextWiller)

tweets_with_retweet_number_upd <- tweets_with_retweet_number %>%
  mutate(urls_url = ifelse(as.character(urls_url)=="NULL", NA, sapply(urls_url, tolower))) %>%
  rowwise() %>% 
  mutate(is_affidabile = !any(urls_url %in% siti_non_affidabili))

tweets_with_retweet_number_upd <- tweets_with_retweet_number_upd %>% 
  mutate(sentiment = sentiment(text))

#View(tweets_with_retweet_number_upd)

tweets_with_retweet_number_upd_plot <- tweets_with_retweet_number_upd %>%
  filter(as.Date(created_at)>"2020-04-22")

ggplot(tweets_with_retweet_number_upd_plot) +
  geom_bar(aes(x = sentiment, fill = is_affidabile), position = position_dodge()) +
  labs(title = "Sentiment Analysis", x="Sentiment", y="n° di tweet") +
  scale_x_continuous(breaks=-1:1, labels=c("Negativo (-1)", "Neutrale (0)", "Positivo (1)")) +
  scale_fill_manual(values=c("#f2a71d", "#1da1f2"), name="Tweets", labels=c("Non affidabili","Affidabili")) +
  theme_minimal()

#sprintf("Positivi: %d, Neutrali: %d, Negativi: %d",nrow(tweets_with_retweet_number_upd %>% filter(sentiment==1, !is_affidabile)), 
#      nrow(tweets_with_retweet_number_upd %>% filter(sentiment==0, !is_affidabile)), 
#      nrow(tweets_with_retweet_number_upd %>% filter(sentiment==-1, !is_affidabile)))


```

In entrambe le categorie di notizie, le più condivise sono quelle con un titolo o una descrizione negativa.
Questo rispetta quello che è stato scoperto da varie ricerche (es. [articolo della BBC](https://www.bbc.com/future/article/20140728-why-is-all-the-news-bad)), cioè che l'essere umano è più propenso a dare più attenzione a notizie negative che a quelle postive.

La neutralità indica notizie che sono postate solamente con il titolo e quest'ultimo non aveva particolari connotazioni negative o positive

L'analisi del sentiment attraverso un vocabolario è sempre da prendere con le pinze, perché ad esempio non considera il sarcasmo o insiemi di parole, in più il testo con cui lavora è molto poco: una, due frasi al massimo per tweet. In ogni caso ritengo sia comunque affidabile visto i risultati ottenuti

```{r Controllo sentimento (manuale), eval=FALSE, include=FALSE}
View(tweets_with_retweet_number_upd %>% 
       select(text, is_affidabile, sentiment) %>%
       filter(is_affidabile, sentiment==0)
     )
```

##CONTESTO

Come ultima analisi, cerchiamo di capire su che argomenti hanno puntato le fake news e se hanno seguito il trend delle notizie più in voga del momento

```{r}
library(tidytext)
library(wordcloud)
library(tm)

stop_words_extra = c("t.co","https","lkhl0j1pqm","wgyypsdiqo","lkhl0ike2c","tpm3unabk9","amp","2","uc8qrmsbx2","weieh1n3qy","wxsqwmbw67","33iqc7xzjo","fispjyrtyw","bgfnwd339h","rxmeve8hzy","lty4aseqyc","wdfy92x5rd","ykmiyz3fm3")
stop_words_vector= append(stop_words_extra, stopwords(kind = "it"), after = length(stop_words_extra))
stop_words_ita = data.frame(word = stop_words_vector)


text_mining_f_tweets <- tweets_with_retweet_number_upd %>%
  filter(!is_affidabile) %>%
  unnest_tokens(word, text) %>%
  select(-urls_expanded_url) %>%
  anti_join(stop_words_ita)

text_mining_t_tweets <- tweets_with_retweet_number_upd %>%
  filter(is_affidabile) %>%
  unnest_tokens(word, text) %>%
  select(-urls_expanded_url) %>%
  anti_join(stop_words_ita)

#Il select è necessario perché dovuto ad un bug nella versione di dplyr 1.0.0 (funziona correttamente nella versione 0.8.5: https://stackoverflow.com/questions/62609723/must-subset-elements-with-a-valid-subscript-vector-during-an-dplyranti-join)

text_mining_f_tweets %>%
  count(word, sort = TRUE) %>%
  filter(n>20) %>%
  with(wordcloud(word, n, max.words = 30, rot.per = 0, random.order=FALSE, random.color=FALSE, colors=brewer.pal(8, "Dark2")))

text_mining_t_tweets %>%
  count(word, sort = TRUE) %>%
  filter(n>20) %>%
  with(wordcloud(word, n, max.words = 30, rot.per = 0, random.order=FALSE, random.color=FALSE, colors=brewer.pal(8, "Dark2")))
  

```

Entrambi i tipi di notizia si focalizzano sulla questione COVID-19 e hanno argomenti molto simili tra di loro. Le fake news però si concentrano più sulla politica e i rapporti con gli altri paesi ("MES", "Trump", "Germania", "Cina", etc...), mentre le altre notizie più sul virus in sé e sull'effetto in italia ("Coronavirus", "Lombardia", "Fase", etc...)
Con questo possiamo dire che più che seguire passo-passo il trend delle notizie verificate, le fake news hanno sfruttato altri avvenimenti contemporanei per fare più click


```{r}

```

